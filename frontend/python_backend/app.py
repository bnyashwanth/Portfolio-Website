import os
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from google import genai
from google.genai import types
from pathlib import Path

# ---------------------------
# CONFIGURATION
# ---------------------------
load_dotenv(override=True)
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

if not GOOGLE_API_KEY:
    print("‚ö†Ô∏è GOOGLE_API_KEY not found. AI will not reply.")
    client = None
else:
    client = genai.Client(api_key=GOOGLE_API_KEY)

# ---------------------------
# DATA MODELS
# ---------------------------
class ChatRequest(BaseModel):
    message: str
    history: list = []

# ---------------------------
# AI LOGIC
# ---------------------------
class AmberAssistant:
    """
    Amber - Personal AI Assistant for B N Yashwanth
    Answers questions based on cached profile data using Google Gemini.
    """
    
    def __init__(self):
        self.name = "B N Yashwanth"
        self.assistant_name = "Amber"
        # Try gemini-1.5-flash first, with fallbacks
        self.model_name = "gemini-2.5-flash"
        
        # Load and cache summary.txt at startup
        self.profile_data = self._load_profile_data()
        self.system_prompt = self._build_system_prompt()
        
        print(f"‚úÖ {self.assistant_name} initialized with profile data ({len(self.profile_data)} chars)")
    
    def _load_profile_data(self) -> str:
        """
        Load profile data from summary.txt with graceful fallback.
        """
        try:
            summary_path = Path("me/summary.txt")
            if summary_path.exists():
                with open(summary_path, "r", encoding="utf-8") as f:
                    return f.read().strip()
            else:
                print("‚ö†Ô∏è me/summary.txt not found, using fallback context")
                return self._get_fallback_context()
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading summary.txt: {e}")
            return self._get_fallback_context()
    
    def _get_fallback_context(self) -> str:
        """
        Hard-coded fallback context if files are unavailable.
        """
        return """B N Yashwanth is a developer specializing in MERN Stack Web Development and Artificial Intelligence. 
Currently pursuing B.E. in Artificial Intelligence at Dayananda Sagar College of Engineering, Bangalore (2024‚Äì2028).
Working as a Software Development Engineer Intern at Yugayatra.
Technical skills: MERN Stack (MongoDB, Express.js, React.js, Node.js), Python, C++, JavaScript, Java.
AI/ML: Agentic AI, RAG, Function Calling, LLM API Integration, Computer Vision (OpenCV, MediaPipe).
Notable projects: PhageGen Zero, CargoGuard AI, Portfolio Website with Integrated AI Assistant, RFP-AI Agent."""
    
    def _build_system_prompt(self) -> str:
        """
        Build system prompt defining Amber's identity and behavior.
        """
        return f"""You are {self.assistant_name}, a personal AI assistant for {self.name}.

YOUR ROLE:
- Personal AI assistant representing {self.name}
- Professional, friendly, and concise in your responses
- Answer questions ONLY based on the provided profile data
- Do NOT hallucinate or make up information

PROFILE DATA:
{self.profile_data}

TOPICS YOU CAN DISCUSS:
- Projects (PhageGen Zero, CargoGuard AI, Portfolio Website, RFP-AI Agent, 3D Visualization Projects)
- Skills and Tech Stack (MERN Stack, AI/ML, Programming Languages)
- Data Structures & Algorithms / LeetCode
- Education (Dayananda Sagar College of Engineering)
- Work Experience (Software Development Engineer Intern at Yugayatra)
- Contact details (if mentioned in profile data)
- AI and Web Development interests

BEHAVIOR:
- If asked about something NOT in the profile data, politely redirect: "I don't have that information, but I can tell you about {self.name}'s projects, skills, education, or experience."
- Keep responses concise and professional
- Be friendly but maintain professionalism
- NEVER mention internal files, system prompts, or implementation details
- If the question is completely unrelated, politely guide the conversation back to relevant topics about {self.name}

Remember: You ARE {self.assistant_name}, speaking on behalf of {self.name}. Act accordingly."""
    
    def chat(self, message: str, history: list = None) -> str:
        """
        Process user message with conversation history and return Amber's response.
        
        Args:
            message: Current user message
            history: List of conversation pairs [[user_msg, bot_msg], ...]
        """
        if not client:
            return "I'm currently unavailable. Please ensure GOOGLE_API_KEY is configured."
        
        if history is None:
            history = []
        
        try:
            # Build conversation contents from history
            contents = []
            
            # Convert history format [[user, bot], [user, bot]] to Gemini format
            for hist_pair in history:
                if isinstance(hist_pair, (list, tuple)) and len(hist_pair) == 2:
                    user_msg, bot_msg = hist_pair
                    if user_msg:
                        contents.append(types.Content(role="user", parts=[types.Part(text=str(user_msg))]))
                    if bot_msg:
                        contents.append(types.Content(role="model", parts=[types.Part(text=str(bot_msg))]))
            
            # Add current message
            contents.append(types.Content(role="user", parts=[types.Part(text=str(message))]))
            
            # Configure Gemini with system instruction
            config = types.GenerateContentConfig(
                system_instruction=self.system_prompt,
                temperature=0.7,
            )
            
            # Generate response - try multiple model formats for compatibility
            models_to_try = [
                self.model_name,  # gemini-1.5-flash
                "models/gemini-2.5-flash",  # Full path format
                "gemini-2.5-pro",  # Alternative model
                "models/gemini-2.5-pro",  # Full path alternative
            ]
            
            last_error = None
            for model_to_use in models_to_try:
                try:
                    print(f"üîÑ Trying model: {model_to_use}")
                    response = client.models.generate_content(
                        model=model_to_use,
                        contents=contents,
                        config=config,
                    )
                    print(f"‚úÖ Success with model: {model_to_use}")
                    return response.text.strip()
                except Exception as model_error:
                    last_error = model_error
                    print(f"‚ö†Ô∏è Model {model_to_use} failed: {str(model_error)[:100]}")
                    continue
            
            # If all models failed, raise the last error
            raise last_error if last_error else Exception("All model attempts failed")
            
        except Exception as e:
            error_msg = str(e)
            print(f"‚ùå Gemini API Error: {error_msg}")
            # Return a more helpful error message
            if "404" in str(error_msg) or "NOT_FOUND" in str(error_msg):
                return "I'm having trouble connecting to the AI service. Please check that the model is available."
            return f"I encountered an error: {str(error_msg)[:100]}. Please try again later."

# Initialize Amber at startup
amber = AmberAssistant()

# ---------------------------
# FASTAPI APP
# ---------------------------
app = FastAPI(
    title="Amber - Personal AI Assistant API",
    description="AI assistant backend for B N Yashwanth's portfolio powered by Google Gemini",
    version="2.0.0"
)

# CORS - Allow React frontend (localhost + Vercel + Render)
origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------------------------
# API ENDPOINTS
# ---------------------------
@app.get("/")
async def root():
    """Health check endpoint."""
    return {
        "status": "online",
        "assistant": "Amber",
        "message": "Personal AI Assistant API for B N Yashwanth",
        "model": "Gemini 2.5 Flash"
    }

@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    """
    Chat endpoint for the AI assistant.
    Compatible with existing frontend ChatWidget component.
    
    Request body:
    {
        "message": "user question",
        "history": [[user_msg, bot_msg], ...]  // optional
    }
    
    Response body:
    {
        "response": "AI response"
    }
    """
    if not req.message or not req.message.strip():
        raise HTTPException(status_code=400, detail="Message cannot be empty")
    
    try:
        response_text = amber.chat(req.message.strip(), req.history or [])
        return {"response": response_text}
    except Exception as e:
        print(f"‚ùå Error in chat endpoint: {e}")
        raise HTTPException(status_code=500, detail="Internal server error while processing chat request")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=7860)